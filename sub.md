Всем привет! Мы с вами тут 
активно разбираем нейронные сети,  
но сегодня я бы предложил отправиться в 
прошлое и выяснить, когда появились первые  
нейронные сети и с чего вообще начиналось 
глубокое обучение. Мы отправимся дальше,  
чем вы могли себе представить и проследим 
важные исторические тенденции, которые  
помогут нам понять текущую картину и даже 
аккуратно поспекулировать на тему будущего.Итак, наша история начинается в 1940-е 
годы. Вы можете сразу спросить - если  
глубокому обучению столько лет, почему оно на 
слуху только последние лет десять? Дело в том,  
что глубокое обучение имело много названий, 
и именно как "глубокое обучение" оно стало  
известно только в 2006 году. До этого - с 
80-х по 90-е оно называлось "коннекционизмом",  
а еще раньше - с 40-х по 60-е - "кибернетикой". 
Вы могли заметить пробелы между этими датами.  
Эти годы соответствуют двум кризисам, во время 
которых по разным причинам наблюдался резкий  
спад популярности нейронных сетей и связанных 
исследований. Эти периоды называют "зимами".
Но обо всем по порядку. Первые 
модели нейронных сетей были  
предложены пионерами эпохи кибернетики еще 
в 40-е. Сама кибернетика сформировалась под  
влиянием нейронаук - исследований нейронов и 
синаптических связей. А первые работы в области  
кибернетики были направлены на построение 
упрощенных математических моделей работы мозга,  
что потенциально позволило бы получить 
первые модели искусственного интеллекта.Успех кибернетики обеспечила 
статья "Логическое исчисление идей,  
относящихся к нервной активности", вышедшая в 
43-ем году. В ней Уоррен Мак-Каллок и Уолтер  
Питтс опубликовали свою модель нейрона. Она 
была не первой, но авторы ввели несколько  
удачных упрощений, которые позволили другим 
исследователям взять их модель за основу.Нейрон Мак-Каллока-Питтса представлял 
собой элементарную логическую единицу.  
Он мог находиться либо в активном, либо в 
неактивном состоянии и передавать сигналы  
другим нейронам. Все сигналы были 
единичными, но у каждого нейрона  
был свой порог активации. Если сумма входных 
сигналов его превышала, нейрон активировался.  
Кроме стандартных, возбуждающих связей 
были ингибирующие. Передача сигнала по  
такой связи предотвращала активацию нейрона 
вне зависимости от суммы других сигналов.В этой же работе Мак-Каллок и Питтс привели 
доказательство того, что с помощью конечной  
сети нейронов можно выполнить любую логическую 
операцию и привели несколько примеров. Посмотрим  
на самые простые. Чтобы выполнить с помощью 
нейронов операцию "А и B", нужно использовать  
две возбуждающие связи и установить порог 
активации нейрона "С" равный двум - обозначим  
его как "тета". Тогда "С" активируется 
только если оба нейрона "А" и "В" активны.  
Для операции "A или B" достаточно снизить порог 
активации "С" до единицы. Тогда он активируется,  
если активен хотя бы один из нейронов, или если 
активны оба. Операции отрицания можно реализовать  
с помощью ингибирующих связей. Например, 
для операции "А и не В" достаточно изменить  
связь "В" с "С" на ингибирующую. "С" активируется 
только если активирован "А" и не активирован "В".Вы можете увидеть тут аналогию с работой 
транзистора. Транзистор - тоже элементарная  
логическая единица, которая может пребывать только 
в одном из двух состояний, а благодаря грамотному  
объединению транзисторов в цепи можно создать 
процессор, способный выполнять любые вычисления.  
Аналогия становится еще более интересной, если 
учесть, что первые транзисторы разрабатывали  
примерно в то же время. Но Мак-Каллок и Питтс 
интересовались только идеями из области чистой  
логики, а не электротехники. Питтс вообще 
видел мозг как идеальную систему нейронов,  
занятую центральной обработкой информации, 
поступающей от органов чувств как от сенсоров.
Но главное, что вы можете заметить в это работе - 
она не имеет ничего общего с машинным обучением. У  
модели должна быть возможность вывести на основе 
данных зависимость, что позволит использовать  
модель для решения подобных задач. Это достигается 
за счет обучаемых параметров и алгоритма обучения.  
Но в статье нет ни того, ни другого. Тем не менее 
нейрон Мак-Каллока-Питтса приковал к себе внимание  
других исследователей, которые пытались 
построить на его основе обучаемые модели.И первая такая модель появилась спустя 15 лет. 
Ее опубликовал молодой ученый Фрэнк Розенблатт  
в 58-ом году в статье под названием "Перцептрон 
- вероятностная модель хранения и организации  
информации в мозге". Розенблатту удалось 
объединить несколько наиболее удачных идей  
ранних исследователей и создать по сути первую 
обучаемую нейронную сеть. Перцептрон был моделью  
из нескольких сотен нейронов Мак-Каллока-Питтса 
с некоторыми важными модификациями.Вместо произвольной конфигурации перцептрон имел 
четкую структуру, состоявшую из нескольких слоев.  
Тут важный момент - в отличие от современных 
нейронных сетей в перцептроне каждый нейрон не  
связывался со всеми нейронами в соседнем слое. 
Это противоречило исследованиям о работе мозга,  
которые показывали избыточность 
такого количества связей. Кроме того,  
это привело бы к лишним вычислениям. 
Вместо этого каждый нейрон связывали  
с некоторым количеством случайных 
нейронов из числа ближайших соседей.Модель нейрона Мак-Каллока-Питтса предполагала 
наличие у каждого нейрона своего порога  
активации и простые связи, которые просто 
передавали сигналы между нейронами без  
изменений. Вместо этого Розенблатт установил 
одинаковый порог активации для всех нейронов,  
но сделал все связи взвешенными. Веса 
связей были обучаемыми параметрами.Наконец, Розенблатт предложил простое правило 
обучения. Пока предположим, что нейроны слева  
содержат входные данные. Так как перцептрон был 
моделью классификации, в последнем слое был один  
или несколько нейронов, отвечавших за определенный 
класс. Если нужный выходной нейрон не срабатывал,  
все связи между ним и активированными нейронами 
во входном слое одинаково усиливались. Ожидалось,  
что в следующий раз взвешенной суммы их сигналов 
уже будет достаточно для активации. Если выходной  
нейрон, наоборот, активировался по ошибке, все 
связи между ним и активированными нейронами во  
входном слое одинаково ослаблялись. Это правило применялось ко всем нейронам в выходном слое.Но самое удивительное в перцептроне было даже не 
то, что он был полноценной обучаемой нейронной  
сетью в 58-ом году, а то, какие задачи он 
решал и как был реализован. Перцептрон был  
моделью компьютерного зрения. В качестве 
входных данных использовались изображения  
размером 20×20 точек. Между входными и выходными 
нейронами был промежуточный слой из нескольких  
сотен нейронов. Он связывался со входными 
нейронами случайными связями с фиксированными  
весами. Так как обучаемыми были только веса 
между промежуточными и выходными нейронами,  
технически это все еще была однослойная 
нейронная сеть. Но наличие промежуточного слоя  
позволяло перцептрону анализировать не отдельные 
пиксели, а сочетания случайных групп пикселей.  
Более поздние модели зрения будут развивать 
ту же идею. В процессе обучения изображения  
обрабатывались по одному, и веса перцептрона 
обновлялись после каждого изображения. Разные  
версии перцептрона могли различать 
до 8 классов объектов. Например,  
перцептрон отлично справлялся с 
классификацией геометрических фигур.
Кроме того, перцептрон не был программой. 
Это был реальный, физический компьютер "Mark  
I Perceptron" с собственным мануалом на 65 
страниц. Веса моделировались потенциометрами,  
которые автоматически настраивались с 
помощью системы моторов в процессе обучения.После такого яркого старта можно было бы ожидать 
бурное развитие и усложнение моделей. И в какой-то  
степени это действительно произошло: и до, и после 
публикации перцептрона шла активная работа над  
другими моделями, но это продолжалось недолго. 
В 69-ом году бывший сокурсник Розенблатта Марвин  
Минский вместе с Сеймуром Пейпертом опубликовал 
книгу "Перцептроны", которая нанесла тяжелый удар  
по кибернетике. В этой работе Минский и Пейперт 
не только критиковали известные на тот момент  
модели, но и в целом ставили под сомнение 
целесообразность использования аналогий из  
нейронауки в искусственном интеллекте. Их идеалом 
был искусственный интеллект, построенный на четких  
логических правилах, а перцептроны 
Минский называл спекулятивной биологией.Но особенно болезненным был небольшой 
пример, который Минский и Пейперт привели,  
для того чтобы подчеркнуть один важный недостаток 
перцептронов. Этот недостаток был незаметен,  
когда данных было много или когда данные 
имели большую размерность - как в случае с  
изображениями. Минский и Пейперт напротив, 
взяли очень простую задачу. Пусть у нас  
есть две переменные A и B, которые могут 
принимать только два значения - истина или  
ложь. Если перцептрон может решать настолько 
сложные задачи, как классификация изображений,  
вряд ли у него будут проблемы с тем, чтобы выучить 
зависимость XOR - логическое исключающее "или". В  
отличие от обычного "или" результат равен 
нулю, когда обе переменные равны единице.Чтобы понять, в чем здесь может быть проблема, 
изобразим задачу графически. Отложим значения  
A и B по осям х и у, а результат операции 
обозначим цветом. Так как мы оказались в двумерном  
пространстве, у нас есть возможность увидеть, 
как принимается решение. По сути перцептрон  
просто умножает входные данные на некоторые 
коэффициенты и сравнивает результат с порогом  
активации. На плоскости это эквивалентно тому, 
чтобы провести прямую и отнести все объекты по  
одну сторону от прямой к одному классу, а по 
другую сторону - к другому. В пространстве с  
большим количеством размерностей вместо этого 
границей будет плоскость или гиперплоскость.И вы уже могли заметить, в чем проблема 
XOR - в отличие от других операций,  
например логического "и" и логического 
"или" - разделить ее результаты с помощью  
одной прямой линии невозможно. Чувствуете, 
как магия перцептрона улетучивается? Для  
его идеальной работы все классы 
должны быть линейно разделимыми.
Тут важно заметить, что все сказанное 
относится только к однослойным перцептронам,  
которые являются одной из разновидностей 
линейных моделей. Решающая граница  
многослойного перцептрона будет состоять из 
полигонов и сможет изолировать нужные точки.  
Так как уже в оригинальном перцептроне 58-го 
года Розенблатт пытался добавить больше слоев,  
хотя и с частично зафиксированными связями, вы 
можете спросить, что мешало ему пойти дальше?
Проблема была в алгоритме обучения. Он позволял 
изменять веса связей только в последнем  
слое - именно они напрямую влияли на ответ. Исходя 
из этого их можно было усилить или ослабить, чтобы  
привести к оптимальному состоянию. Но каким могло 
быть оптимальное состояние связей в предыдущих  
слоях, которые оказывали только опосредованное 
влияние на результат, было абсолютно непонятно.Все это контрастировало с амбициозными 
заявлениями Розенблатта о потенциальных  
возможностях перцептронов. При этом влияние 
работы Минского и Пейперта было столь велико,  
что интерес к перцептронам начал угасать. Сам 
Розенблатт трагически погиб всего два года спустя.  
А один из авторов оригинальной модели нейрона - 
Уолтер Питтс исчез из научной жизни еще раньше.  
Он был потрясен статьей "Что видит глаз лягушки", 
в которой сам принимал участие. Эта статья  
доказывала, что глаз не является просто сенсором. 
Паттерны света анализируются еще в сетчатке,  
и в мозг поступает уже в некоторой степени 
осмысленная информация о наличии объектов и  
движения, а не просто об уровне освещенности. 
Это пошатнуло взгляды Питтса и его видение  
мозга как идеальной логической машины для 
централизованной обработки информации. После  
выхода статьи Питтс уничтожил свои исследования 
и фактически прекратил свою научную деятельность.Другие исследователи вслед за Минским 
обращались к альтернативным подходам и моделям,  
финансирование кибернетики резко сократилось 
и началась первая зима. Вместе с этим могла  
бы окончиться и история нейронных 
сетей, которые навсегда остались бы  
"перцептронами" - интересными с исторической 
точки зрения, но бесполезными моделями.
Но ирония в том, что всего через 5 
лет после публикации "Перцептронов"  
аспирант Пол Вербос опубликовал в своей 
докторской диссертации тот самый недостающий  
алгоритм обучения многослойных моделей - 
алгоритм обратного распространения ошибки. В оригинале он назывался "backpropagation" или 
просто "backprop". По сути он был объединением  
градиентного спуска и правила 
дифференцирования сложных функций.
Вообще градиентный спуск - это универсальный 
алгоритм приближенного поиска минимума функции,  
который используют, когда точные методы 
оказываются неэффективными. Применительно  
к перцептронам градиентный спуск можно 
использовать для поиска параметров,  
которые минимизируют отклонение модели от истинных 
значений на выборке обучения. Поиск минимума в  
градиентном спуске выполняется с помощью серии 
шагов, на каждом из которых вычисляют производную  
ошибки по каждому из параметров модели. Ее 
значение характеризует вклад этого параметра  
в ошибку. Для уменьшения ошибки нужно вычесть из 
текущего значения параметра значение производной,  
умноженное на небольшое число. Серия таких шагов 
приведет параметры к оптимальному состоянию,  
которое соответствует минимальному 
отклонению модели от истинных значений.Этот алгоритм звучит далеко не так понятно, как 
правило обучения Розенблатта, но у него есть  
значительное преимущество. Для градиентного 
спуска, в отличие от правила Розенблатта,  
нет разницы между параметрами в последнем 
слое, которые напрямую влияют на ответ,  
и параметрами в более глубоких слоях 
модели - их влияние в любом случае  
оценивается через производные. Но есть 
разница в плане вычисления - производные  
по параметрам в глубоких слоях 
вычислять труднее. Именно для  
их поиска Вербос и предложил использовать 
правило дифференцирования сложных функций.Попробуем вычислить производные отклонения 
модели по параметрам `w1` и `w2`. Правило  
дифференцирования сложных функций заключается 
в том, чтобы выразить сложные производные через  
произведение более простых производных, в 
данном случае рассматривая работу модели в  
обратном направлении. Например, отклонение 
модели напрямую зависит от предсказания,  
а предсказание напрямую зависит от `w2`, так как 
этот параметр находится в последнем слое. Для  
вычисления производной по `w1` понадобится на один 
шаг больше. Используем ту же логику - отклонение  
модели зависит от предсказания, предсказание 
зависит от `x2`, а `x2` - от `w1`. Если вы не  
уверены, что наши умозрительные заключения вообще 
корректны, можете просто зачеркнуть одинаковые  
множители и убедиться, что выражения слева и 
справа совпадают. Значения производных справа  
зависят от функции потерь и функции активации, 
и посчитать их, как правило, нетрудно. Остается  
рассчитать аналогичным образом производные по 
всем параметрам и повторить циклы предсказаний  
и коррекции весов достаточное количество раз - 
пока отклонение модели не достигнет минимума.Работа Вербоса была настолько удачной, что должна 
была изменить положение вещей и дать новый толчок  
развития кибернетики, но этому помешало 
стечение обстоятельств. Вербос занимался  
социальными науками и использовал алгоритм для 
обучения не перцептрона, а похожей многослойной  
модели. Так как научное сообщество на тот момент 
не могло так активно обмениваться информацией,  
этот алгоритм прошел мимо других исследователей. 
О нем снова вспомнят только спустя 12 лет.Но в том же году Кунихико Фукусима 
опубликовал еще одну важную работу.  
В ней был описан когнитрон - модель, которая 
была способна различать объекты на изображении,  
так же как и перцептрон Розенблатта. Но в 
отличие от перцептрона она насчитывала 4 слоя  
с обучаемыми параметрами, и Фукусиме удалось 
обучить ее вообще не используя алгоритмы,  
основанные на коррекции ошибок. 
Вместо этого он использовал подход,  
похожий на обучение без учителя, а процесс 
обучения называл "самоорганизацией".На вход когнитрона подавалось изображение 
размером 12×12 точек. Все слои когнитрона  
имели такое же количество нейронов. Каждый 
нейрон соединялся с нейроном прямо напротив и  
еще с несколькими нейронами поблизости случайным 
образом. Так же как и в перцептроне Розенблатта,  
у каждой связи был вес, который изменялся в 
процессе обучения. Но в отличие от перцептрона,  
нейроны Фукусимы были небинарными - их сигналы 
были непрерывными числами, зависящими от суммы  
входных сигналов. Позже от этого базового 
свойства нейрона Мак-Каллока-Питтса откажутся  
и другие исследователи, но Фукусиме это нужно 
было для реализации правила "самоорганизации".В ответ на каждое изображение нейроны 
активировались с разной силой. Алгоритм  
Фукусимы делил каждый слой на небольшие участки 
и на каждом из участков одинаково усиливал  
связи только одного нейрона - того, который 
активировался сильнее остальных. Этот процесс  
повторялся в каждом слое и позволял постепенно 
формировать нейроны, которые специализировались  
на определенных паттернах в изображении 
и игнорировали все остальные. Например,  
отдельные нейроны могли отвечать за углы, 
вертикальные или горизонтальные участки линий.  
Нейроны в более поздних слоях реагировали 
уже на уникальные комбинации паттернов на  
более крупных участках изображения, 
например на геометрические формы.А исходя из анализа распределения наиболее 
сильных активаций в финальном слое можно  
было классифицировать объект 
на изображении. Предположим,  
что на единицы чаще всего реагировали вот эти 
нейроны, на двойки - эти, а на тройки - эти.  
Тогда если новое изображение привело к такому 
паттерну активации, то скорее всего на нем двойка.Следующие значительные успехи 
появились только спустя 12 лет,  
и это были не просто удачные статьи, а скорее 
абсолютно новый взгляд на нейронные сети,  
который лучше совпадал с духом нового времени. 
Дело в том, что за годы первой зимы произошел  
значительный сдвиг в научных исследованиях. 
Основное внимание было приковано не только  
к устройству мозга и процессов в нейронах - 
этим занимались нейронауки - но и изучению  
принципов мышления, памяти и сложного 
поведения человека. Этим занималась новая  
дисциплина - когнитивистика. Иными словами, 
вместо ответа на вопрос "как устроен мозг",  
когнитивистика пыталась найти ответы на 
вопросы "как мы думаем, запоминаем и учимся".В 86-ом году Дэвид Румельхарт, Джеффри Хинтон 
и соавторы опубликовали сразу две работы,  
благодаря которым стало возможным не 
только обучение многослойных моделей,  
но и получение ответов на многие вопросы 
когнитивистики. Первой работой был двухтомник  
"Параллельное распределенное представление" 
или PDP. Это довольно объемный труд,  
но к его основным положениям можно отнести 
видение процессов мышления и памяти как сложных  
паттернов активации нейронов. Этот процесс 
прослеживался в обученных авторами моделях  
и мог быть успешно экстраполирован на работу 
мозга. Следуя же классическому искусственному  
интеллекту за который выступали Минский и 
Пейперт, когнитивистам пришлось бы поверить,  
что большая часть нейронов в мозге отвечает 
за хранение и распознавание сложных,  
цельных образов - например животных или 
предметов - что было намного менее вероятно.С вычислительной точки зрения это значило, что 
достаточно глубокие модели в процессе обучения  
способны без явных инструкций выделять в сложных 
объектах различные признаки - такие, как размер,  
форма, цвет или углы - и делать предсказание 
на основе обнаруженных комбинаций имеющихся и  
отсутствующих признаков. Вы могли увидеть 
в этом параллели с когнитроном Фукусимы,  
и это неслучайно. Фукусима также 
вдохновлялся работами ранних когнитивистов.Второй работой Румельхарта, Хинтона и соавторов 
была небольшая статья "Изучение репрезентаций с  
помощью обратного распространения 
ошибки", вышедшая в том же году.  
В ней они продемонстрировали алгоритм обратного 
распространения ошибки, который открыли повторно  
независимо от Пола Вербоса, сделавшего это, 
как мы помним, еще в 74-ом году. Позднее авторы  
признали за Вербосом первенство в открытии. Но 
примечательно то, что в своей работе они применили  
этот алгоритм к обучению многослойных перцептронов 
- раньше, как мы помним, их обучение считалось в  
принципе невозможным. Авторы также окончательно 
отказались от идеи бинарных нейронов, которые  
мешали реализации алгоритма обучения. Простую идею 
порога активации, просуществовавшую еще со времен  
нейрона Мак-Каллока-Питтса, заменили сигмоидальной 
активацией, при которой нейрон плавно переходил из  
неактивного состояния в активное. В отличие от 
пороговой функции, она была дифференцируемой,  
а так как алгоритм обучения наполовину состоял 
из вычисления производных, это было критично.С решением проблемы обучения многослойных 
моделей, которая в свое время оказалась  
одним из катализаторов первой зимы, и 
новыми открытиями о работе перцептронов,  
глубокое обучение получило второй подъем 
под новым названием "коннекционизм". Оно  
лучше отражало суть подхода: создание 
и обучение моделей на основе большого  
количества связанных элементарных 
вычислительных единиц - нейронов.В этот период также окончательно 
сформировались две новые архитектуры  
моделей - сверточные нейронные сети для 
обработки изображений и рекуррентные  
нейронные сети для обработки естественного 
языка и других последовательностей. Работы,  
в которых они были впервые опубликованы, 
довольно схожи - их авторы внесли меньший вклад,  
чем предыдущие исследователи, но именно их 
версия в итоге закрепилась и стала стандартом.Предлагаю начать со сверточных нейронных сетей. 
Их первую версию предложил Ян ЛеКун в 89-ом году  
в своей работе "Влияние архитектуры нейронных 
сетей на генерализацию". В ней он подчеркнул  
недостатки ранних моделей зрения, таких 
как Перцептрон Розенблатта и Когнитрон  
Фукусимы. Они были схожи в том плане, что каждый 
нейрон соединялся с точками изображения набором  
обучаемых связей. Это позволяло каждому нейрону 
реагировать на определенный паттерн в определенной  
области изображения. Но у этой модели был явный 
недостаток - одни и те же паттерны - например,  
углы, линии или замкнутые фигуры - могли 
появляться в разных частях изображения,  
и некоторый обмен знаний между нейронами мог 
бы сделать модель эффективнее. Кроме того,  
небольшой сдвиг или искажение изображения могли 
значительно понизить точность модели, так как  
в область видимости большинства нейронов могли 
попасть совершенно незнакомые для них паттерны.Для решения этих проблем ЛеКун предложил 
использовать несколько видоизмененную модель  
из более поздних работ Фукусимы. Ее основная 
идея заключалась в том, чтобы отвязать детекцию  
паттерна от позиции за счет синхронизации 
весов всех нейронов в одном слое. Кроме того,  
ЛеКун сделал связи более структурированными - 
каждый нейрон получал информацию не от случайного  
набора ближайших точек, а от каждой точки в 
небольшом окне размером 3х3 или 5х5 точек. Так  
как веса каждого нейрона были одинаковыми, все 
нейроны реагировали на один и тот же паттерн,  
но на своем участке изображения. А 
чтобы детектировать несколько паттернов,  
нужно было просто увеличить количество слоев. Веса 
нейронов в новом слое также были одинаковыми, но  
отличались от весов в других слоях, что позволяло 
каждому слою реагировать на свой паттерн.Нейроны в более глубоких слоях соединялись с 
соответствующим окном в каждом из предыдущих  
слоев и реагировали на уникальные 
комбинации извлеченных паттернов. Слоев  
на этом уровне также могло быть несколько для 
определения разных комбинаций. Финальный слой,  
выполнявший классификацию, был похож на слой 
перцептрона. Он связывался со всеми нейронами  
предыдущих слоев и выносил решение исходя из 
анализа комбинации паттернов на каждой позиции.Итоговая модель представляла собой один из 
ранних вариантов сверточной нейронной сети.  
Она была нечувствительна к сдвигу и при этом 
имела намного меньше обучаемых параметров,  
чем перцептрон. Для обучения модели, как 
вы могли догадаться, ЛеКун использовал  
алгоритм обратного распространение 
ошибки. В процессе обучения более  
ранние слои обучались поиску наиболее важных 
паттернов, которые помогали решить задачу,  
а финальный слой - выносить решение 
исходя из их наличия или отсутствия.В той же работе ЛеКун сравнил 
точность классификации  
рукописных цифр с помощью разных моделей. 
Результаты модели, которую мы рассмотрели,  
в последней строке - она достигла невероятной 
точности в 98.4%. Для сравнения перцептрон,  
наиболее близкий к модели в первой строке, 
верно классифицировал всего 80% объектов. Причем  
перцептрону не помогло бы даже наличие обучаемых 
скрытых слоев - это модель во второй строке.Теперь поговорим о втором классе моделей - 
рекуррентных нейронных сетях. К их появлению  
привело несколько особенностей задач обработки 
текста и других сложных последовательностей,  
из-за которых существующие модели не 
могли адекватно с ними справиться.Текст, в отличие от других типов данных, имеет 
четкую последовательную структуру. Изменение  
последовательности может полностью поменять 
смысл. Кроме того, при анализе текста нужно  
запоминать различные факты, которые могут быть 
необходимы для понимания более позднего контекста.
Одним из вариантов решения этих проблем 
было добавление в нейронные сети циклических  
связей. Это позволило бы обрабатывать 
текст последовательно, обновляя на  
каждом шаге внутренне состояние модели. Это 
состояние могло бы содержать общее понимание  
текущего контекста и наиболее важные факты из 
прошлых итераций, то есть работать как память.  
Это настолько старая идея, что ее можно отследить 
еще к оригинальной работе Мак-Каллока-Питтса,  
где циклы были одним из возможных 
типов связей между нейронами.Позже такие модели стали называться рекуррентными. 
Многие исследователи развивали эту концепцию и  
предлагали новые версии моделей, но наиболее 
каноничной считается версия Джеффри Элмана,  
описанная в статье "Поиск структуры во 
времени", опубликованной в 90-ом году.В этой работе Элман продемонстрировал,  
что рекуррентные нейронные сети способны 
справляться не только с генерацией текста,  
но и обобщать синтаксическую и семантическую 
структуру языка без явных инструкций. Но  
прежде чем перейти к самой модели, рассмотрим 
задачу, которую решал Элман более подробно.
Словарь в языке Элмана был упрощенным - он состоял 
всего из 29 слов и пары спецсимволов. Все слова  
кодировались с помощью 31-битных векторов, где 
только один бит был равен единице. Таким образом  
часть речи, род, возможный контекст и любая другая 
информация о каждом слове полностью отбрасывалась.С помощью словаря Элман создал 
корпус синтетического текста,  
состоящего из простых предложений. Тем 
не менее текст не был настолько простым,  
чтобы любое пропущенное слово можно 
было однозначно понять из контекста.  
Как и в обычном языке, контекст позволял 
только ограничить круг возможных слов.
Этот корпус использовался для обучения модели. 
Модель получала по одному слову за раз и должна  
была предсказать следующее слово. Обычная 
нейронная сеть с одним скрытым слоем в лучшем  
случае могла бы обучиться запоминать фразы из 
двух слов. Поэтому для того чтобы у модели была  
возможность работать с более длинным контекстом, 
Элман добавил еще один скрытый рекуррентный слой,  
который был связан со скрытым слоем и 
- с помощью циклической связи - сам с  
собой. Циклические связи были зафиксированы 
и равны единице. Таким образом, информация  
после каждого обработанного слоя добавлялась в 
память без изменений и постепенно формировала  
общий контекст. А связи между скрытыми слоями 
были обучаемыми - так модель могла выбирать,  
что из контекста важно в данный момент. 
В обоих слоях было по 150 нейронов.Как вы могли догадаться, модель обучалась с 
помощью алгоритма обратного распространения  
ошибки. Точность модели была достаточно 
высокой - насколько это возможно в задачах,  
где присутствует неопределенность. Но 
удивительнее всего было не это. Память  
модели была достаточно большой, чтобы запомнить 
несколько предыдущих слов последовательности,  
но анализ активаций скрытого слоя показал, что для 
сжатия информации и более эффективной генерации  
текста модель сформировала и использовала 
синтаксическую и семантическую модель языка.
Разница между активациями модели на различные 
слова следовала четкой структуре - все глаголы  
имели схожие паттерны активации, так же 
как и все существительные. Внутри этих  
групп можно модель выделяла группы со схожими 
значениями слов и даже более мелкие группы.Но несмотря на все успехи, взлет коннекционизма 
продлился не больше 10 лет. Иронично,  
но его развитию помешали почти те же причины, 
что привели к спаду кибернетики. Во-первых,  
то, что появился способ обучать 
многослойные модели, еще не значило,  
что слоев может быть сколько угодно. На 
практике даже модели всего из 3-4 слоев  
были практически необучаемыми. К этому 
приводила комбинация негативных факторов,  
которые на тот момент еще не были обнаружены. 
Но 
даже от тех моделей, которые поддавались обучению,  
было все еще сложно добиться адекватных 
результатов из-за нехватки данных и  
слабого железа. Большинство наборов данных не 
находилось в открытом доступе. Каждая группа  
исследователей собирала данные самостоятельно и 
редко могла выйти за пределы нескольких сотен или  
тысяч объектов. Но даже при таком количестве 
данных и довольно скромных размерах моделей,  
для компьютеров того времени объем вычислений 
был слишком большим. Да, процессоры уже давно  
не строились на системе электронных ламп - 
такие успел застать разве что Розенблатт,  
но их производительность все еще была ничтожной 
современным меркам. Я сделаю непростительную  
вещь и просто сравню мегагерцы в лоб, но просто 
для осознания - первое поколение Intel Pentium,  
вышедшее в 95-ом году, имело тактовую частоту 
в районе 60 мегагерц - это в 4 раза ниже,  
чем у современного копеечного микроконтроллера 
ESP32. И Яну ЛеКуну, например, потребовалось трое  
суток на обучение крошечной по современным меркам 
сверточной нейронной сети на железе такого уровня.Во-вторых, большие успехи первых работ снова 
заставили исследователей делать преждевременно  
амбициозные заявления. Кто-то говорил даже о 
скором создании почти человеческого интеллекта.  
И инвесторы, которые поначалу были в восторге, но 
по итогу не получили быстрых результатов, начали  
сворачивать финансирование. Все это сопровождалось 
активным развитием других моделей и подходов.  
Примерно в то же время появились машины опорных 
векторов и ядерные методы. Вы могли слышать о них  
как о не самых популярных на данный момент моделях 
из области Classic ML. Но в то время им удалось  
показать отличные результаты - в основном за счет 
более низких требований как к количеству данных,  
так и к железу. Все это привело к 
началу новой зимы с середины 90-х.Но по счастью, в этот раз удар по исследованиям не 
был настолько сокрушительным. Работы продолжались,  
а Канадскому институту продвинутых 
исследований - CIFAR - даже удалось  
объединить ученых из разных университетов 
под одной исследовательской программой.
Эта группа вместе с другими исследователями 
продолжала улучшать существующие модели и  
открывала новые подходы. В 97-ом году вышла работа 
по LSTM - улучшенной версии рекуррентных нейронных  
сетей с дополнительным треком для долгосрочной 
памяти. Он позволял моделям работать с текстами  
и другими последовательностями намного большей 
длины, что раньше было большой проблемой.Годом позже, в 98-ом Ян ЛеКун собрал 
немыслимый для того времени по размерам  
и качеству набор данных MNIST, состоявший из 70 
тысяч рукописных символов. Им пользуются до сих  
пор в качестве бенчмарка для новых моделей. Он 
также непрерывно улучшал сверточные нейронные  
сети в последующих работах на протяжении 10 лет. 
Одна из последних итераций его модели - LeNet-5,  
вышедшая также в 98-ом году, практически 
не отличалась от современных моделей.Но финальным шагом, после которого 
произошел прорыв, стала работа,  
опубликованная Джеффри Хинтоном в 2006 году. 
В ней он представил новую генеративную модель  
под названием Deep Belief Network. 
Но нам важна не сама модель, а то,  
что в ней было четыре слоя, и Хинтону удалось 
ее обучить. Его стратегия была разновидностью  
жадного алгоритма и заключалась в обучении 
модели по одному слою за раз, параметры  
остальных слоев при этом фиксировались. После 
этого модель обучалась еще раз, но уже целиком.  
Алгоритм Хинтона показал отличную сходимость 
и позволил обучать более глубокие модели. Но,  
как всегда, дело было не только в этой работе. За 
время второй зимы произошло два важных изменения,  
которые позволили исследователям 
прийти к третьему и финальному подъему.Первое, как ни странно - это бурное развитие 
игровой индустрии. Практически в один год с  
выходом работы Хинтона появились такие 
легенды как Assassin's Creed, BioShock,  
Crysis, Portal и Mass Effect. Требования 
игр постоянно росли, а работа с графикой  
была настолько тяжелой, что выполнялась 
отдельно от просчета мира на видеокартах,  
которые были оптимизированы под параллельную 
обработку тысяч графических примитивов в реальном  
времени. Налаженное производство и постоянное 
улучшение видеокарт для рынка игр позволило  
исследователям использовать быстрые графические 
ядра, но уже для параллельной обработки тензоров,  
и на порядки ускорить обучение нейронных сетей. 
Наибольший вклад здесь внесла NVidia - еще  
в 2007-ом она выпустила фреймворк CUDA для 
вычислений общего назначения на видеокартах, затем  
линейки специализированных видеокарт с тензорными 
ядрами, а начиная с 20-ой серии тензорные ядра  
появились и в видеокартах потребительского 
сегмента. В играх они используются для DLSS.Второе важное изменение, которое поспособствовало 
подъему - резкий рост объема доступных данных. С  
развитием интернета готовые наборы данных вышли 
за пределы университетов, а количество доступной  
информации росло практически экспоненциально. 
Набор данных MNIST, созданный в 98-ом году под  
руководством Яна ЛеКуна, был 
прорывом для своего времени,  
но его 70 тысяч изображений ничто по сравнению 
с 10 миллионами изображений в наборе ImageNet,  
появившемся всего 10 лет спустя. Его создание 
потребовало колоссального объема ручной работы,  
но оно было бы в принципе невозможным без 
наличия такого количества изображений в  
интернете. А современным генеративным моделям 
разметка в некоторых случаях вообще не требуется.  
Поэтому так же как Google и Яндекс в свое время 
без спроса скачали и проиндексировали большую  
часть доступных на тот момент страниц в интернете 
для создания поисковиков, IT компании сейчас без  
спроса выкачивают содержимое YouTube, GitHub 
и других ресурсов для обучения своих моделей.
Два этих обстоятельства вместе с работой 
Хинтона, которая показала возможность  
обучения по-настоящему сложных моделей, 
вместе послужили началом новой эпохи.  
Хинтон впервые начал использовать термин "глубокое 
обучение" в своих работах, подчеркивая сложность  
и количество слоев в новых моделях. 
Название закрепилось, и все задачи,  
так или иначе связанные с нейронными 
сетями, так называют до сих пор.С этого момента открытия происходили так 
часто, что многие подходы успевали пройти  
полный цикл от появления и полного доминирования 
до полной утраты актуальности всего за пять лет.  
Лучше всего это проиллюстрирует тот факт, что 
алгоритм Хинтона, ставший катализатором прогресса,  
перестал использоваться уже примерно к 2012-му 
году. С переходом на более удачные алгоритмы  
случайной инициализации весов, функции активации 
без насыщения - такие, как ReLU - и более сложные  
алгоритмы оптимизации вроде Adam, модели 
стали сходиться без дополнительных действий.Предлагаю коротко пройтись по основным 
достижениям в ключевых областях - без  
фамилий и подробных описаний. Так как 
речь пойдет про современные модели,  
вы либо уже о них знаете, 
либо еще будете их изучать.
Начнем с компьютерного зрения. Работы в 
этой области показали настоящий потенциал  
масштабирования моделей. После довольно небольших 
моделей из 4-5 слоев, разработанных ЛеКуном на  
рубеже 90-х и нулевых, следующим прорывом стала 
AlexNet, опубликованная в 2012-ом. Она состояла из  
8 слоев и была одной из первых моделей, обученных 
на видеокартах, что дало ей огромное преимущество.  
Уже через два года, в 2014-ом, AlexNet превзошла 
новая модель - VGG, состоявшая из 19 слоев. А в  
2015-ом была опубликована ResNet, авторы которой 
смогли довести глубину до 152 слоев. Позже,  
в 2019-ом авторы новой модели EfficientNet 
показали, что эффект от увеличения количества  
слоев может быть усилен, если грамотно 
сбалансировать его с увеличением разрешения  
изображения и количества сверточных фильтров, и 
в очередной раз превзошли все предыдущие модели.И если успехи сверточных нейронных сетей 
были значительными и позволяли фокусироваться  
в основном на преодолении ограничений 
масштабирования и небольших доработках уже  
существующей архитектуры, с обработкой текста дела 
обстояли гораздо хуже. Несмотря на ранние успехи,  
долгое время ни LSTM, ни другие рекуррентные 
модели не могли эффективно справляться с длинными  
текстами. Но именно эти неудачи и привели, 
возможно, к самому важному прорыву в истории  
глубокого обучения на текущий момент. Я говорю о 
работе 2017 года - "Все что нужно - это внимание",  
которая предложила не-рекуррентный подход 
работы с последовательностями и открыла  
эпоху новых моделей - трансформеров. Трансформеры 
оказались настолько удачными и универсальными,  
что позволили не только создать 
модели обработки и генерации текста,  
включая современные LLM агенты вроде 
ChatGPT, но и перейти на территорию  
компьютерного зрения. Модель ViT, опубликованная 
в 2020-ом году, несмотря на отличные результаты,  
все еще была спорной из-за огромного количества 
параметров и затрат на обучение по сравнению со  
сверточными моделями. Но последующие трансформеры 
окончательно превзошли сверточные модели и до сих  
пор продолжают доминировать как в задачах 
зрения, так и в задачах обработки языка.Сейчас трансформеры продолжают активно 
масштабироваться, хотя и не такими взрывными  
темпами, как раньше. Новые модели становятся 
больше, точнее, лучше на основных бенчмарках - но  
это все больше напоминает выход новой линейки 
iPhone. По-настоящему амбициозной следующей  
целью сейчас является AGI, или искусственный 
общий интеллект. Одни исследователи считают,  
что AGI вполне достижим с помощью нейросетей, 
и что основное ограничение - это объем данных  
и размеры моделей. Другие уверены, что для 
настоящего AGI потребуются принципиально новые  
подходы и идеи. Тем временем регулярно звучат 
обещания близости AGI - то в конце 2023 года, то  
в 2024, теперь вот в 2026. Тут легко провести 
параллели с заявлениями Розенблатта и перегревом  
ожиданий в эпоху коннекционизма. И совершенно 
логично предположить - если станет очевидно,  
что AGI недостижим в ближайшие 
годы, нас может ждать новая зима.На самом деле эти опасения преувеличены - и в 
целом судьба большинства рядовых ML инженеров и  
Data Scientist-ов слабо зависит от AGI. Все потому 
что глубокое обучение уже успело интегрироваться в  
экономику, технологии и повседневную жизнь. 
Если еще в 90-е одним из лучших достижений  
нейронных сетей была возможность оцифровки 
рукописных индексов на письмах, то сегодня на  
них построена работа крупнейших компаний мира - от 
рекомендательных систем в стриминговых сервисах и  
маркетплейсах до автономных автомобилей. И даже 
если революции не случится, эти алгоритмы будут  
продолжать развиваться еще долгие годы. Ну а если 
AGI все-таки будет достигнут - тогда нас с вами  
ждет Skynet. Хотя, если честно, мне кажется, 
будущее скорее будет похоже на "Идиократию".Еще один важный момент, который хотелось 
бы обсудить - это аналогия между нейронными  
сетями и работой мозга. Сегодня мы много о 
ней говорили, причем эта аналогия работала в  
обе стороны - сначала кибернетика вдохновлялась 
работой нейронов, потом достижения кибернетики  
и коннекционизма использовались для построения 
гипотез о работе мозга. Но на сегодняшний день  
важно понимать - эти две области давно пошли 
разными путями. Математическое моделирование  
работы мозга - это отдельная дисциплина со 
своим аппаратом, задачами и моделями. Эти  
модели создают не для предсказания роста акций на 
фондовом рынке или генерации изображений — их цель  
исключительно в понимании нейрофизиологии 
и когнитивных процессов. Нейронные сети,  
в свою очередь, сегодня имеют очень поверхностное 
сходство с реальными моделями мозга. Более того,  
если вы проследите происхождение ключевых идей 
последних лет - например, трансформеров или  
диффузионных моделей - то увидите, что они 
берутся из других областей. А многие ранние  
биологические аналогии - вроде ступенчатых функций 
активации - наоборот, последовательно вычищаются.  
Хотя нельзя исключать, что в будущем открытия 
о работе мозга дадут новые идеи для глубокого  
обучения - возможно, даже для создания AGI - 
на данный момент это две очень далекие области.На этом предлагаю заканчивать. Надеюсь вам было 
интересно пройти весь этот путь вместе со мной  
и познакомиться с нашими героями. Немногим из них 
посчастливилось увидеть, к чему приведут их идеи,  
но именно благодаря им все наши сегодняшние 
достижения оказались возможными. Многих  
исследователей я упомянул здесь только словом 
"соавторы", а многих не упомянул совсем,  
чтобы вы не потерялись в потоке фамилий. В любом 
случае, спасибо за просмотр, а если вам интересна  
не только история нейронных сетей, заходите 
на канал. Увидимся в код ревью, всем пока!